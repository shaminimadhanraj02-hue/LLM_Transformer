# ğŸ¤– LLM Experiments using Hugging Face Transformers

This repository documents my learning journey with **Large Language Models (LLMs)** through the **Hugging Face Transformers** library.  
All experiments were created and run inside **GitHub Codespaces** using Python.

---

## ğŸ§­ Project Overview

This repo contains beginner-friendly implementations of foundational LLM tasks:
- **Zero-Shot Classification** â€” classify unseen text into candidate labels without training  
- **Masked Word Prediction (Fillâ€‘Mask)** â€” predict missing words using BERT/RoBERTa  
- **Named Entity Recognition (NER)** â€” identify people, organizations, and locations in text  
- **Text Generation** â€” generate continuations with causal language models (e.g., GPTâ€‘2)

Each task demonstrates how to use the `transformers` pipeline, interpret results, **and fix common errors** I actually hit while learning.

---

## ğŸ“‚ Folder Structure

```
LLM_Transformer/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ zero_shot.py                 # Zero-shot text classification
â”‚   â”œâ”€â”€ mask_filling.py              # Masked word prediction
â”‚   â”œâ”€â”€ ner_recognition.py           # Named Entity Recognition
â”‚   â”œâ”€â”€ text_generation.py           # Text generation with GPT-2
â”‚   â””â”€â”€ results/                     # Saved results and output files
â”‚       â”œâ”€â”€ zero_shot_result.txt
â”‚       â”œâ”€â”€ mask_filling_result.txt
â”‚       â”œâ”€â”€ ner_result.txt
â”‚       â””â”€â”€ text_generation_result.txt
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

> Results are saved under `src/results/` so the code and its outputs live together.

---

## ğŸ§  Projects Completed

### 1ï¸âƒ£ Zeroâ€‘Shot Classification
**File:** `src/zero_shot.py`

```python
from transformers import pipeline

classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

result = classifier(
    "The Germany's weather is so unpredictable",
    candidate_labels=["education", "climate", "confused", "culture"],
)

print(result)
```
**Output (example)**
```
"labels": ["climate", "confused", "culture", "education"]
"scores": [0.87, 0.08, 0.03, 0.01]
```

**Common error & fix**
- `ModuleNotFoundError: No module named 'transformers'` â†’ `pip install transformers torch` inside the **active venv**.

Saved to: `src/results/zero_shot_result.txt`

---

### 2ï¸âƒ£ Masked Word Prediction (Fillâ€‘Mask)
**File:** `src/mask_filling.py`

```python
from transformers import pipeline
unmasker = pipeline("fill-mask", model="bert-base-cased")   # uses [MASK]

result = unmasker("In Germany, Oktoberfest happens in this [MASK].", top_k=3)
print(result)
```
**Output (example)**
```
In Germany, Oktoberfest happens in this month.  â†’ 0.865
In Germany, Oktoberfest happens in this city.   â†’ 0.734
In Germany, Oktoberfest happens in this festival. â†’ 0.622
```

**Common errors & fixes**
- `PipelineException: No mask_token ([MASK]) found on the input`  
  â†’ I used `<mask>` by mistake; **BERT expects `[MASK]`**. (RoBERTa would use `<mask>`.)  
- `OSError: No space left on device` (Codespaces cache full)  
  â†’ `rm -rf ~/.cache/huggingface` then rerun to redownload only whatâ€™s needed.  
- Trailing comma created a tuple accidentally: `result = unmasker(...),`  
  â†’ remove the comma: `result = unmasker(...)`.

Saved to: `src/results/mask_filling_result.txt`

---

### 3ï¸âƒ£ Named Entity Recognition (NER)
**File:** `src/ner_recognition.py`

```python
from transformers import pipeline
ner = pipeline(
    "ner",
    model="dbmdz/bert-large-cased-finetuned-conll03-english",
    aggregation_strategy="simple"   # replaces deprecated grouped_entities=True
)
print(ner("I am Shamini studying masters at FAU in Erlangen."))
```
**Output (example)**
```
Shamini â†’ PER (Person)
FAU â†’ ORG (Organization)
Erlangen â†’ LOC (Location)
```

**Notes**
- Warning about `grouped_entities` being deprecated â†’ fixed by using `aggregation_strategy="simple"`.
- The â€œSome weights â€¦ not used (pooler)â€ message is **normal** for token classification.

Saved to: `src/results/ner_result.txt`

---

### 4ï¸âƒ£ Text Generation  âœ… *(added)*
**File:** `src/text_generation.py`

```python
from transformers import pipeline

# Use a causal LM for generation (not BART-MNLI which is for classification)
generator = pipeline("text-generation", model="gpt2")

prompt = "In Germany, Oktoberfest is"
outputs = generator(prompt, max_length=40, num_return_sequences=1, do_sample=True)
print(outputs[0]["generated_text"])
```

**Common pitfalls I fixed**
- âŒ Using a nonâ€‘generation model (e.g., `facebook/bart-large-mnli`) with `"text-generation"` â†’  
  **Fix:** Use a causal LM like `"gpt2"` or `"EleutherAI/gpt-neo-125M"` for generation tasks.
- âŒ Accidental trailing comma: `generator = pipeline(...),` â†’ tuple, then `TypeError` when calling â†’ **remove comma**.
- âŒ â€œNo module named torchâ€ â†’ `pip install torch` in the **venv**.
- â„¹ï¸ First run is slow while the model downloads; later runs use the cache.

Saved to: `src/results/text_generation_result.txt`

---

## ğŸ’¡ How I Learned & Fixed Errors (Cheatâ€‘Sheet)

| Challenge | What I Did |
|---|---|
| Virtual env not active | `source .venv/bin/activate` before installing/running |
| Missing packages | `pip install transformers torch` |
| Wrong mask token | `[MASK]` for BERT, `<mask>` for RoBERTa |
| â€œFetch firstâ€ on `git push` | `git pull --rebase origin main` then `git push` |
| Disk full in Codespaces | `rm -rf ~/.cache/huggingface` |
| Trailing comma turned value into tuple | Remove trailing commas in assignments/calls |
| Deprecation: `grouped_entities` | Use `aggregation_strategy="simple"` |

---

## âš™ï¸ Install & Run

```bash
git clone https://github.com/shaminimadhanraj02-hue/LLM_Transformer.git
cd LLM_Transformer

python3 -m venv .venv
source .venv/bin/activate            # Windows: .venv\Scripts\Activate.ps1

pip install transformers torch

# Run any task
python src/zero_shot.py
python src/mask_filling.py
python src/ner_recognition.py
python src/text_generation.py
```

Results are written to `src/results/` (text/JSON), so code and outputs live together.

---

## ğŸ’¾ Git Workflow I Follow

```bash
git add src/
git commit -m "Add/Update LLM experiment + saved results"
git push origin main

# If push is rejected (remote ahead):
git pull --rebase origin main
git push origin main
```

---

## ğŸŒŸ Learnings
- Core pipelines in `transformers` and when to choose each model family.  
- Matching the **right task** to the **right model** (classification vs generation).  
- Practical debugging in Codespaces (venv, caches, git rebase).  

---

## ğŸš€ Next
- Summarization & Questionâ€‘Answering pipelines  
- A tiny RAG demo over personal notes  
- Optional: connect a simple LLM service to ROSÂ 2

---

## ğŸ§¾ Credits
- [Hugging Face Transformers Docs](https://huggingface.co/docs/transformers)
- [Hugging Face LLM Course](https://huggingface.co/learn/llm-course)

---

## ğŸ‘©â€ğŸ’» Author
**Shamini Madhanraj** â€” Masterâ€™s student (Germany) exploring AI, Robotics & LLMs.
